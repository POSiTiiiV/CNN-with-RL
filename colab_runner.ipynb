{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with RL Hyperparameter Optimization - Colab Runner\n",
    "\n",
    "This notebook allows you to run the CNN-with-RL project on Google Colab.\n",
    "\n",
    "## Step 1: Check GPU Availability\n",
    "\n",
    "First, let's check if a GPU is available. You can change the runtime type to GPU in Colab by going to:\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPU(s): {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU instead. Training will be much slower.\")\n",
    "    print(\"Consider changing the runtime to GPU in Runtime > Change runtime type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone the Repository\n",
    "\n",
    "Now we'll clone the CNN-with-RL repository from GitHub. If your repository is private, you may need to [set up SSH keys or personal access tokens](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual repository URL)\n",
    "!git clone https://github.com/yourusername/CNN-with-RL.git\n",
    "# For private repositories, use:\n",
    "# !git clone https://username:personal_access_token@github.com/yourusername/CNN-with-RL.git\n",
    "\n",
    "# Change to the repository directory\n",
    "%cd CNN-with-RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "Install required packages from requirements.txt or manually specify them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# If requirements.txt doesn't exist, install common packages manually\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pandas numpy matplotlib seaborn scikit-learn pillow wandb tqdm pyyaml tensorboard opencv-python gym stable-baselines3 psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Download and Prepare Dataset\n",
    "\n",
    "Set up the dataset directory. In Colab, you can:\n",
    "1. Upload data from your local machine\n",
    "2. Download from Google Drive\n",
    "3. Download from another source\n",
    "\n",
    "Here we'll demonstrate the Google Drive approach since it's most convenient for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "!mkdir -p data/merged_images\n",
    "!mkdir -p data/csv_files\n",
    "\n",
    "# Option 1: Copy data from Google Drive (if you have data stored there)\n",
    "# Replace with your actual paths in Google Drive\n",
    "!cp -r /content/drive/MyDrive/path/to/dataset/merged_images/* data/merged_images/\n",
    "!cp -r /content/drive/MyDrive/path/to/dataset/csv_files/* data/csv_files/\n",
    "\n",
    "# Option 2: Download data from URL\n",
    "# !wget -O dataset.zip https://example.com/dataset.zip\n",
    "# !unzip dataset.zip -d data/\n",
    "\n",
    "# Check if data was correctly copied/downloaded\n",
    "!ls -la data/merged_images | head -5\n",
    "!ls -la data/csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Modify Configuration (Optional)\n",
    "\n",
    "You can view and modify the configuration file if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the default configuration\n",
    "!cat configs/default.yaml\n",
    "\n",
    "# Uncomment and modify to create a custom configuration\n",
    "'''\n",
    "import yaml\n",
    "\n",
    "# Load the default config\n",
    "with open('configs/default.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Modify parameters\n",
    "config['training']['batch_size'] = 16\n",
    "config['training']['epochs'] = 10\n",
    "config['data']['num_workers'] = 2\n",
    "\n",
    "# Save the custom config\n",
    "with open('configs/custom.yaml', 'w') as file:\n",
    "    yaml.dump(config, file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Weights & Biases (Optional)\n",
    "\n",
    "If you want to use Weights & Biases for experiment tracking, log in here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Weights & Biases (optional)\n",
    "import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run the Training Script\n",
    "\n",
    "Now we'll run the main.py script with appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training script\n",
    "# Uncomment and adjust the command as needed\n",
    "\n",
    "# Basic run with default configuration\n",
    "!python main.py --data-dir \"data/merged_images\"\n",
    "\n",
    "# Extended run with more options\n",
    "# !python main.py \\\n",
    "#     --config \"configs/default.yaml\" \\\n",
    "#     --data-dir \"data/merged_images\" \\\n",
    "#     --output-dir \"./output\" \\\n",
    "#     --wandb \\\n",
    "#     --wandb-project \"CNN-with-RL-Colab\" \\\n",
    "#     --wandb-name \"colab-run-1\" \\\n",
    "#     --seed 42 \\\n",
    "#     --epochs 10 \\\n",
    "#     --batch-size 16 \\\n",
    "#     --device \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Monitor Training Progress\n",
    "\n",
    "While training is running, you can monitor system resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU usage\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Results\n",
    "\n",
    "After training completes, you can visualize results from the latest run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Find the latest results file\n",
    "latest_run = max(glob.glob('output/run_*'), key=os.path.getctime)\n",
    "results_file = os.path.join(latest_run, 'results.json')\n",
    "\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Run directory: {latest_run}\")\n",
    "    print(f\"Best validation accuracy: {results['best_val_accuracy']:.4f}\")\n",
    "    print(f\"Best validation loss: {results['best_val_loss']:.4f}\")\n",
    "    print(f\"Test accuracy: {results['test_acc']:.4f}\")\n",
    "    print(f\"Test loss: {results['test_loss']:.4f}\")\n",
    "    print(f\"Training time: {results['training_time']:.2f} seconds\")\n",
    "    print(f\"Total epochs: {results['epochs']}\")\n",
    "    print(f\"Early stopped: {results['early_stopped']}\")\n",
    "    print(f\"RL interventions: {results['rl_interventions_count']}\")\n",
    "else:\n",
    "    print(f\"Results file not found at {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Results\n",
    "\n",
    "Finally, you can download the trained models and results to your local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the output directory for easy download\n",
    "!zip -r results.zip output/\n",
    "\n",
    "# Enable downloading the results\n",
    "from google.colab import files\n",
    "files.download('results.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
