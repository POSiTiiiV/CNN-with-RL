# CNN Model Configuration
verbose: true  # Enable verbose output for detailed logging

model:
  in_channels: 3
  num_classes: 8  # Will be automatically updated based on dataset
  
# Training Configuration
training:
  batch_size: 64  # Increased from 32 to better utilize GPU memory
  learning_rate: 0.001
  epochs: 30
  max_epochs: 200
  early_stopping_patience: 100 # TODO: remember to change it back to maybe 30
  checkpoint_frequency: 5
  min_epochs_before_intervention: 5
  intervention_frequency: 5
  stagnation_threshold: 0.005
  use_mixed_precision: true  # Enable mixed precision training for better GPU utilization
  
# Reinforcement Learning Configuration
rl:
  learning_rate: 0.0003
  episodes: 50
  steps_per_episode: 5
  max_steps_per_episode: 5
  n_steps: 2048  # Number of steps to collect before updating policy
  batch_size: 64  # Minibatch size for policy update
  n_epochs: 10  # Number of epochs when optimizing the surrogate loss
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda parameter
  clip_range: 0.2  # PPO clipping parameter
  normalize_advantage: true  # Whether to normalize advantages
  brain_save_dir: "models/rl_brains"  # Directory to save RL models
  # Intervention decision parameters
  intervention_threshold: 0.4  # Threshold for deciding when to intervene (lower = less interventions)
  ent_coef: 0.01  # Entropy coefficient to encourage exploration
  # Dynamic training parameters
  train_frequency_min: 5  # Minimum frequency for training updates
  train_frequency_max: 25  # Maximum frequency for training updates 
  training_timesteps_min: 500  # Minimum timesteps per training update
  training_timesteps_max: 10000  # Maximum timesteps per training update
  min_results_for_training: 5  # Minimum frequency for training updates
  max_results_for_training: 25  # Maximum frequency for training updates
  # RL brain saving configuration
  rl_brain_save_frequency: 5  # How often to save the brain (every N decisions)
  
# Environment Configuration
env:
  hyperparameter_ranges:
    learning_rate: [0.0001, 0.01]  # Min, Max values
    dropout_rate: [0.1, 0.9]
    weight_decay: [1e-6, 1e-2]
  
# Data Configuration
data:
  dataset_name: "custom"  # Changed to use custom dataset
  train_val_split: 0.8
  image_size: 224  # Size for resizing images (suitable for ResNet34)
  num_workers: 6  # Increased from 4 to improve data loading throughput
  # Image caching options
  cache_images: false  # Disabled PIL image caching to save memory
  cache_tensors: true  # Cache transformed tensor images (preferred)
  use_global_cache: true  # Share cache across train/val/test datasets
  minimal_transform: true  # Use minimal transformations for preprocessed images
  # DataLoader optimization
  persistent_workers: true # Keep worker processes alive between epochs
  warmup_loaders: true     # Warm up loaders before training starts
  prefetch_factor: 4       # Increased from 2 to prefetch more batches per worker
  # Memory management
  max_cache_size_gb: 4     # Maximum cache size in GB
  
# Logging Configuration
logging:
  log_dir: "logs"
  save_model_dir: "models"
  tensorboard_dir: "logs/tensorboard"
  use_wandb: true
  
# Weights & Biases Configuration
wandb:
  project: "CNN-with-RL"
  run_name: null  # Will be auto-generated based on timestamp if null
  log_interval: 1  # Log every N epochs

# TODO: update default.yaml file to have correct and all configs