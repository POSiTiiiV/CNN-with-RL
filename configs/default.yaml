# CNN Model Configuration
verbose: true  # Enable verbose output for detailed logging

model:
  in_channels: 3
  num_classes: 8  # Will be automatically updated based on dataset
  
# Training Configuration
training:
  batch_size: 64  # Increased from 32 to better utilize GPU memory
  learning_rate: 0.001
  epochs: 30
  max_epochs: 100
  early_stopping_patience: 70 # TODO: remember to change it back to maybe 30
  checkpoint_frequency: 5
  # Increased minimum epochs before first intervention from 5 to 10
  min_epochs_before_first_intervention: 10
  # Increased epochs between interventions from 5 to 15
  epochs_between_interventions: 15
  # Minimum epoch counts for proper evaluation
  min_epochs_per_evaluation: 5
  stagnation_threshold: 0.005
  use_mixed_precision: true  # Enable mixed precision training for better GPU utilization
  
# Reinforcement Learning Configuration
rl:
  learning_rate: 0.0003
  episodes: 50
  steps_per_episode: 5
  max_steps_per_episode: 5
  n_steps: 2048  # Number of steps to collect before updating policy
  batch_size: 64  # Minibatch size for policy update
  n_epochs: 10  # Number of epochs when optimizing the surrogate loss
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95  # GAE lambda parameter
  clip_range: 0.2  # PPO clipping parameter
  normalize_advantage: true  # Whether to normalize advantages
  brain_save_dir: "models/rl_brains"  # Directory to save RL models
  # Intervention decision parameters
  intervention_threshold: 0.6  # Increased threshold for less frequent interventions (higher = more conservative)
  ent_coef: 0.005  # Reduced entropy coefficient to reduce exploration aggressiveness
  # Penalties for frequent major interventions
  major_intervention_penalty: 0.5  # Penalty for major interventions
  consecutive_major_interventions_penalty: 1.0  # Additional penalty for consecutive major interventions
  # Dynamic training parameters
  train_frequency_min: 5  # Minimum frequency for training updates
  train_frequency_max: 25  # Maximum frequency for training updates 
  training_timesteps_min: 500  # Minimum timesteps per training update
  training_timesteps_max: 10000  # Maximum timesteps per training update
  min_results_for_training: 5  # Minimum frequency for training updates
  max_results_for_training: 25  # Maximum frequency for training updates
  # RL brain saving configuration
  rl_brain_save_frequency: 5  # How often to save the brain (every N decisions)
  
# Environment Configuration
env:
  hyperparameter_ranges:
    # Narrow down hyperparameter ranges to prevent wild swings
    learning_rate: [0.0005, 0.005]  # More constrained range
    dropout_rate: [0.2, 0.5]  # More constrained dropout
    weight_decay: [1e-5, 1e-3]  # More constrained weight decay
  
  # Dynamic step scheduling configuration
  use_dynamic_steps: true
  
  # Epochs per step parameters - increased for better evaluation
  initial_epochs_per_step: 5  # Increased from 3 to 5
  min_epochs_per_step: 3      # Increased minimum to ensure more epochs per evaluation
  max_epochs_per_step: 10     # Increased from 7 to 10
  
  stagnation_patience: 5      # Increased from 2 to 5
  
# Data Configuration
data:
  dataset_name: "custom"  # Changed to use custom dataset
  train_val_split: 0.8
  image_size: 224  # Size for resizing images (suitable for ResNet34)
  num_workers: 6  # Increased from 4 to improve data loading throughput
  # Image caching options
  cache_images: false  # Disabled PIL image caching to save memory
  cache_tensors: true  # Cache transformed tensor images (preferred)
  use_global_cache: true  # Share cache across train/val/test datasets
  minimal_transform: true  # Use minimal transformations for preprocessed images
  # DataLoader optimization
  persistent_workers: true # Keep worker processes alive between epochs
  warmup_loaders: true     # Warm up loaders before training starts
  prefetch_factor: 4       # Increased from 2 to prefetch more batches per worker
  # Memory management
  max_cache_size_gb: 4     # Maximum cache size in GB
  
# Logging Configuration
logging:
  log_dir: "logs"
  save_model_dir: "models"
  tensorboard_dir: "logs/tensorboard"
  use_wandb: true
  
# Weights & Biases Configuration
wandb:
  project: "CNN-with-RL"
  run_name: null  # Will be auto-generated based on timestamp if null
  log_interval: 1  # Log every N epochs

# TODO: update default.yaml file to have correct and all configs